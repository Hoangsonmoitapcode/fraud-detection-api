#!/usr/bin/env python3
"""
Pre-cache ML models during Docker build to reduce startup time
"""

import os
import sys

def cache_models():
    """Pre-download and cache ML models"""
    print("ğŸ”„ Starting model caching...")
    
    try:
        import torch
        print(f"âœ… PyTorch {torch.__version__} loaded")
    except ImportError as e:
        print(f"âŒ PyTorch import failed: {e}")
        return False
    
    try:
        import transformers
        print(f"âœ… Transformers {transformers.__version__} loaded")
    except ImportError as e:
        print(f"âŒ Transformers import failed: {e}")
        return False
    
    # Cache PhoBERT model
    try:
        from transformers import AutoTokenizer, AutoModel
        
        print("ğŸ“¥ Downloading PhoBERT tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained('vinai/phobert-base')
        print("âœ… PhoBERT tokenizer cached")
        
        print("ğŸ“¥ Downloading PhoBERT model...")
        model = AutoModel.from_pretrained('vinai/phobert-base')
        print("âœ… PhoBERT model cached")
        
        # Test the model briefly
        print("ğŸ§ª Testing model...")
        inputs = tokenizer("Xin chÃ o", return_tensors="pt")
        with torch.no_grad():
            outputs = model(**inputs)
        print("âœ… Model test successful")
        
        return True
        
    except Exception as e:
        print(f"âš ï¸ PhoBERT caching failed: {e}")
        print("ğŸ“ Model will be downloaded at runtime")
        return False

if __name__ == "__main__":
    success = cache_models()
    print(f"ğŸ Model caching {'completed' if success else 'finished with warnings'}")
    # Don't exit with error even if caching fails
    sys.exit(0)
